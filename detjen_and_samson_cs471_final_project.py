# -*- coding: utf-8 -*-
"""Detjen and Samson - CS471 Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10jp9vVgZQerM3WpnT8WnII_4kinATylp
"""

!pip install -U transformers

"""## Local Inference on GPU
Model page: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""

## Load and Initialize the model

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")

### Demo question for the base model

messages = [
    {"role": "user", "content": "Write a python function that determines whether a number is prime."},
]
inputs = tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=1000)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

## Import PDF reader tool

!pip -q install pypdf sentence-transformers faiss-cpu transformers accelerate

from pypdf import PdfReader

reader = PdfReader("/content/CS110_Mock_Syllabus.pdf")

pages = [p.extract_text() for p in reader.pages]
doc_text = "\n".join(pages)
print(doc_text[:10000])  # preview first 1k chars

## Function to run the LLM

def generateResponse(prompt, messages):

  messages.append({"role": "user", "content": f"Answer the question using below:\n\nSyllabus:\n{doc_text}\n\nQuestion: " + prompt})

  inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
  ).to(model.device)

  outputs = model.generate(**inputs, max_new_tokens=1000)
  reply = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:])
  print("Response:" + reply + "\n")

  messages.append({"role": "assistant", "content": reply})

### MAIN PROGRAM
# Run this code block to interact with our LLM

runLLM = True
messages = []

print("***** CS110 24/7 Instructor *****\n")

while runLLM:

	prompt = input("Ask any question about CS110: ")


	if (prompt == "quit"):
		runLLM = False

	else:

		generateResponse(prompt, messages)